{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "86534e84",
      "metadata": {
        "id": "86534e84"
      },
      "source": [
        "# Fine-tuning an LLM (Gemma 3) using Quantisation and LoRA (QLoRA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c11119-c6d2-4fc6-b67a-933d2c81319c",
      "metadata": {
        "id": "92c11119-c6d2-4fc6-b67a-933d2c81319c",
        "outputId": "1ed8a8cf-f9a3-4b00-c31e-42421e027ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q -U bitsandbytes\n",
        "%pip install -q -U transformers\n",
        "%pip install -q -U peft\n",
        "%pip install -q -U accelerate\n",
        "%pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8404bd-6517-41ab-99bd-ef91560d3f72",
      "metadata": {
        "id": "5b8404bd-6517-41ab-99bd-ef91560d3f72",
        "outputId": "aec27f9c-9932-4aeb-bc66-12acaef9728d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': \"'Two things are infinite: the universe and human stupidity 100% agree.' -Albert Einstein \\n\\nIf you are planning your 2023 in the US, you are in the right place! In fact, I will explain how to plan your 2023 in the United States, if you are interested in living in the United States. \\n\\nIf you are a graduate student or have just completed your graduation, the first thing to do before you can plan your 2023 in the United States is to secure\"}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"google/gemma-3-1b-pt\"\n",
        "pipe_og = pipeline(\"text-generation\", model=model_id, device=\"cuda\", torch_dtype=torch.bfloat16)\n",
        "output = pipe_og(\"'Two things are infinite: the universe and human stupidity \", max_new_tokens=100)\n",
        "output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914484bd-acef-4c04-9971-ddc9c86f0f8a",
      "metadata": {
        "id": "914484bd-acef-4c04-9971-ddc9c86f0f8a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, torch_dtype=torch.bfloat16)\n",
        "model = model.to(device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f1750b8-6986-4e5b-8bcb-c47667874954",
      "metadata": {
        "id": "7f1750b8-6986-4e5b-8bcb-c47667874954"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d6e3195-f00e-4d78-a9ea-f3086dd5f72f",
      "metadata": {
        "id": "3d6e3195-f00e-4d78-a9ea-f3086dd5f72f"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42467550-8d46-4e6e-aa1e-59cdb6e1e387",
      "metadata": {
        "id": "42467550-8d46-4e6e-aa1e-59cdb6e1e387",
        "outputId": "655dd809-1c7b-4614-8fc0-ad44ba21cddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemma3ForCausalLM(\n",
            "  (model): Gemma3TextModel(\n",
            "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-25): 26 x Gemma3DecoderLayer(\n",
            "        (self_attn): Gemma3Attention(\n",
            "          (q_proj): Linear4bit(in_features=1152, out_features=1024, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=1024, out_features=1152, bias=False)\n",
            "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Gemma3MLP(\n",
            "          (gate_proj): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=6912, out_features=1152, bias=False)\n",
            "          (act_fn): PytorchGELUTanh()\n",
            "        )\n",
            "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
            "    (rotary_emb): Gemma3RotaryEmbedding()\n",
            "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f3270c-e815-4d2d-8610-97551de04435",
      "metadata": {
        "id": "f6f3270c-e815-4d2d-8610-97551de04435",
        "outputId": "63cb96f5-f6e6-4ff6-a7a8-9025338e81d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2981888 || all params: 653986944 || trainable%: 0.4559552797433216\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3695bba3-44f8-4642-9721-c869b70eda76",
      "metadata": {
        "id": "3695bba3-44f8-4642-9721-c869b70eda76",
        "outputId": "ae0ec618-193b-4eab-e183-ef1a4bd99a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 2508\n",
            "})\n",
            "Dataset({\n",
            "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 2000\n",
            "}) Dataset({\n",
            "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 508\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n",
        "dataset = dataset.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
        "print(dataset)\n",
        "train_data = dataset.select(range(2000))\n",
        "val_data = dataset.select(range(2000,2508))\n",
        "print(train_data, val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a2d9d6",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9d73007aa6a54fafa1325d11a004eafd"
          ]
        },
        "id": "31a2d9d6",
        "outputId": "b1cd6ee7-af88-4f85-8ce6-23b66c0cc56d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d73007aa6a54fafa1325d11a004eafd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def preprocess_quotes(example):\n",
        "    return tokenizer(\n",
        "        example[\"quote\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "train_data = train_data.map(preprocess_quotes, batched=False)\n",
        "val_data = val_data.map(preprocess_quotes, batched=False)\n",
        "\n",
        "columns = [\"input_ids\", \"attention_mask\"]\n",
        "train_data.set_format(type=\"torch\", columns=columns)\n",
        "val_data.set_format(type=\"torch\", columns=columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4304f7a2",
      "metadata": {
        "id": "4304f7a2",
        "outputId": "047b5626-d718-4816-8cd4-5306d5a30f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([     2, 236913,   3912,   5869, 236793,   4677,   1663,    563,   3016,\n",
            "          3523,   1827,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
            "             1,      1,      1,      1,      1,      1,      1,      1])\n",
            "tensor([     2, 236913,   3912,   5869, 236793,   4677,   1663,    563,   3016,\n",
            "          3523,   1827,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100])\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "batch = data_collator([train_data[i] for i in range(2)])\n",
        "print(batch[\"input_ids\"][0])\n",
        "print(batch[\"labels\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b094490a",
      "metadata": {
        "id": "b094490a",
        "outputId": "6a9bf631-103b-461c-ee75-ee8507c43f24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.pad_token_id == tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01092a50",
      "metadata": {
        "id": "01092a50",
        "outputId": "1f8f0396-8e85-4678-e093-209608c47347"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 15:05, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.269400</td>\n",
              "      <td>2.668428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.214200</td>\n",
              "      <td>2.679520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.154400</td>\n",
              "      <td>2.699140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.105900</td>\n",
              "      <td>2.724246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.083000</td>\n",
              "      <td>2.732720</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=625, training_loss=2.165372412109375, metrics={'train_runtime': 907.3794, 'train_samples_per_second': 11.021, 'train_steps_per_second': 0.689, 'total_flos': 2.153097068544e+16, 'train_loss': 2.165372412109375, 'epoch': 5.0})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#hyperparameters\n",
        "batch_size = 4\n",
        "lr = 1e-4\n",
        "num_epochs = 5\n",
        "\n",
        "#training loop using API from Hugging Face\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=num_epochs,\n",
        "        learning_rate=lr,\n",
        "        bf16=True,\n",
        "        logging_steps=1,\n",
        "        weight_decay=0.01,\n",
        "        logging_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        report_to=\"tensorboard\",\n",
        "        logging_dir=\"outputs/logs\",\n",
        "\n",
        "    ),\n",
        "    data_collator = data_collator ,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86cd62f6",
      "metadata": {
        "id": "86cd62f6",
        "outputId": "06f6671f-a53c-446d-a5cd-f649e8de7f1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.15.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (4.23.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (75.8.0)\n",
            "Requirement already satisfied: six>1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir outputs/logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85cfdf4a-5fdb-44a4-94b2-f28c49e67c64",
      "metadata": {
        "id": "85cfdf4a-5fdb-44a4-94b2-f28c49e67c64"
      },
      "outputs": [],
      "source": [
        "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(\"outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f94bdc0b",
      "metadata": {
        "id": "f94bdc0b"
      },
      "source": [
        "### Quantitative Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5150f19",
      "metadata": {
        "id": "d5150f19",
        "outputId": "7644f5dd-a93e-4988-bc65-0cc1cbc0e80f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='127' max='127' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [127/127 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity: 14.42\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "eval_loss = eval_results[\"eval_loss\"]\n",
        "perplexity = math.exp(eval_loss)\n",
        "\n",
        "print(f\"Perplexity: {perplexity:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b4d0f7",
      "metadata": {
        "id": "38b4d0f7"
      },
      "source": [
        "For small/medium LMs on simple datasets (quotes), a perplexity between 10 and 50 (14.42) is generally decent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a4f2a1",
      "metadata": {
        "id": "79a4f2a1"
      },
      "source": [
        "### Qualitative Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0277c3-6c1b-440c-98f3-af550616d16f",
      "metadata": {
        "id": "3f0277c3-6c1b-440c-98f3-af550616d16f",
        "outputId": "67a61100-922d-4cea-aeb2-fd2e74592322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finetuned Model example 0 -\n",
            "“The only real prison is fear, and the only real freedom is freedom from fear” – Plato.”This is a lie I cannot bear. In the midst of all our strength we still hide from each other. It is impossible to hold that feeling for long, it never dies” – Anne Frank. “It is a sad fact, the greatest of men find their greatest weakness when they are most needed. It is in crises that they find their greatest courage and honesty.” – Voltaire.â€Žâ€Žâ€Žâ€Žâ€Žâ€Žâ€\n",
            "\n",
            "Original Model example 0 -\n",
            "[{'generated_text': '“The only real prison is fear, and the only real freedom is freedom from fear” - unknown.\\n\\nFreedom is the right to do or believe or to be without restriction in our society. Freedom is also the opportunity to do and say what we wish to do or say in our personal lives.\\n\\nThis blog will provide general information and tips on living a life of freedom.\\n\\nIt’s not to say that you will see all these things in your life, they’re just ideas to get you started.\\n\\nFreedom is a state of mind, a way of thinking, a'}]\n",
            "\n",
            "Finetuned Model example 1 -\n",
            "“I hope you will have a wonderful year, that you'll dream dangerously and outrageously, that you'll make something that didn't exist before you made it, that you will be loved and that you will be liked, and that you will have people to love and to like in return. And, most importantly (because I think there should be more kindness and more wisdom in the world right now), that you will, when you need to be, be wise, and that you will always be kind.” - Unknown - ”I hope you will have a wonderful year, that you'll dream dangerously and outrageously, that you'll make something that didn't exist before you made it, that you will be loved and that you will be liked, and that you will have people to love and to like in return. And, most importantly (because I think there should be more kindness and more wisdom in the world right now), that you will, when you need to be, be wise,\n",
            "\n",
            "Original Model example 1 -\n",
            "[{'generated_text': \"“I hope you will have a wonderful year, that you'll dream dangerously and outrageously, that you'll make something that didn't exist before you made it, that you will be loved and that you will be liked, and that you will have people to love and to like in return. And, most importantly (because I think there should be more kindness and more wisdom in the world right now), that you will, when you need to be, be wise, and that you will always be kind.” ~Sarah Dessen, The Truth About Forever\\n\\n“Let’s stop talking about how much we love each other, and just focus on how much we love us.” ~Sarah Dessen, The Truth About Forever\"}]\n",
            "\n",
            "Finetuned Model example 2 -\n",
            "“It's the job that's never started as takes longest to finish.””Well, you should know me, I'm a liar.””Yes, you are; you lie constantly.”He frowned. “I guess that's the kind of lying I like.””I think it's the lying that's good.””Yes, I see.”They talked over this and that for some time. They both knew that they had never been in love.”This isn't a problem to be solved,\" said Nick, \"It's just a problem\n",
            "\n",
            "Original Model example 2 -\n",
            "[{'generated_text': \"“It's the job that's never started as takes longest to finish.”\\n\\n“The first time you take out a picture, you feel a little nervous because it's a new story and sometimes it's just too much of a story.”\\n\\n“This movie is more than 360 people, which is what I hope it is going to be and hopefully that's what happens.”\\n\\n“This isn't a movie, it's a drama.”\\n\\n“It's a film where the actors are more in character than anybody has seen them\"}]\n",
            "\n",
            "Finetuned Model example 3 -\n",
            "“I am not proud, but I am happy; and happiness blinds, I think, more than pride.””I am not angry, but I am sorry; and sorry, I think, more than angry.””I am not frightened, but I am calm; and calmness comes better than fear.””I am not jealous, but I am proud; and pride comes better than jealousy.””I am not afraid to die; and I am glad, too, when I die. I can't bear to be alive without having loved and been loved, and be free, and not have had hope\n",
            "\n",
            "Original Model example 3 -\n",
            "[{'generated_text': '“I am not proud, but I am happy; and happiness blinds, I think, more than pride.” – David Hume\\n\\nWe were all born on October 10, 1970 at exactly 8:15 AM. Not only was it my 42nd birthday but it also happened to be my first anniversary.\\n\\nAlthough our relationship was very short, I have had the pleasure of being married to this most wonderful man. He has been the most loving, gentle, thoughtful and caring husband anyone could hope for. He made me the most fabulous mom and he was the best'}]\n",
            "\n",
            "Finetuned Model example 4 -\n",
            "“I donâ€™t trust anybody. Not anybody. And the more that I care about someone, the more sure I am theyâ€™re going to get tired of me and take off.” Harry Potter and the Prisoner of Azkaban\n",
            "When we start dating Harry, we all have something special that makes it different. When you love someone, you can do something that is beyond anything normal. You can do something you wouldn't have done if you were ordinary. You can do things to amaze, to amaze someone, to be different, and to create new things. You want to see your other half in every detail, but you realize your flaws. You want to see every color\n",
            "\n",
            "Original Model example 4 -\n",
            "[{'generated_text': '“I donâ€™t trust anybody. Not anybody. And the more that I care about someone, the more sure I am theyâ€™re going to get tired of me and take off.”\\n\\nâ€”George Will, Washington Post\\n\\nOne of the most important questions in this countryâ€™s history has little to do with who â€” or whether â€” won the Civil War. Instead, itâ€™s a question that can be answered with a simple yes or no. The question is whether the country was better or worse after.\\n\\nI have long been convinced that the answer is unequivocally the better. The war was fought, for a start, to preserve slavery, which was the'}]\n",
            "\n",
            "Finetuned Model example 5 -\n",
            "“I can only note that the past is beautiful because one never realises an emotion at the time. It expands later, and thus we don't have complete emotions about the present, only about the past.” -- A.S. Byatt -- The Lay-Soldiers “I want to know why I am here, why I am the way I am.” -- John Fowles -- The Fates “It is a hard thing to say goodbye to your childhood. I mean, you've known and felt and trusted for so long, it is really an awful thing to have to face up to one day. You say, Okay, goodbye, friend, you've been a good friend.\n",
            "\n",
            "Original Model example 5 -\n",
            "[{'generated_text': \"“I can only note that the past is beautiful because one never realises an emotion at the time. It expands later, and thus we don't have complete emotions about the present, only about the past.”\\n\\n- Henry James\\n\\nYou see, I’ve been sitting here watching “the past” for 2 hours now.\\n\\nI could write about this in detail, but you’re here for the music. So, I won’t.\\n\\nI was watching this video, which I found on YouTube. In it, we have the lyrics to some song from 2009 called “We’re Here To Stay”. I’m pretty sure it’s the artist Theophilus\"}]\n",
            "\n",
            "Finetuned Model example 6 -\n",
            "“It takes courage to love, but pain through love is the purifying fire which those who love generously know. We all know people who are so much afraid of pain that they shut themselves up like clams in a shell and, giving out nothing, receive nothing and therefore shrink until life is a mere living death.” ― J. D. Salinger\n",
            "\n",
            "I am grateful for the love that flows through my veins, I am grateful that I can be seen, heard, and admired. I am grateful to have a mind that refuses to fall prey to the mind of a monster. I am grateful for the love of a man who knows me better than I know myself. I am grateful that life is full of love and pain. I am grateful to feel this great sorrow so that I may be truly moved by\n",
            "\n",
            "Original Model example 6 -\n",
            "[{'generated_text': '“It takes courage to love, but pain through love is the purifying fire which those who love generously know. We all know people who are so much afraid of pain that they shut themselves up like clams in a shell and, giving out nothing, receive nothing and therefore shrink until life is a mere living death.”\\n― Robert Browning, The Barchester Towers\\n\\n“When I was a child, I had a fear of being killed. I had a fantasy of being killed by my family, at least, that is the idea I have. I just never believed it would happen, and I never imagined it would happen. But the whole fantasy is gone. They have never hurt me, so I’ve never believed that I might.”\\n― David Foster Wallace, Infinite Jest'}]\n",
            "\n",
            "Finetuned Model example 7 -\n",
            "“Learn from the mistakes of others. You can never live long enough to make them all yourself.” ― Confuciusâ€”â€”â€” â€” “In my own life, I have found that one man's laughter is often another man's cry. I have learned that there are two sides to every story. I have learned that everyone has their own secret and it's easy to hide it. I have learned that laughter can brighten one's day. I have learned that I am human, flawed, sinful, but I have learned that we all deserve a chance to live\n",
            "\n",
            "Original Model example 7 -\n",
            "[{'generated_text': '“Learn from the mistakes of others. You can never live long enough to make them all yourself.”\\n― Albert Einstein, Relativity\\nThis was a huge learning experience for me, because I never really thought about this concept. There are many times in my life I have used my imagination to think about what might happen if a certain scenario did not play out as it did. Most times, I got it right. However, I realized that I often didn’t think of the other side of the situation. I didn’t consider the consequences of what might happen if a person had taken certain actions'}]\n",
            "\n",
            "Finetuned Model example 8 -\n",
            "“Great heroes need great sorrows and burdens, or half their greatness goes unnoticed. It is all part of the fairy tale.” - Roald Dahl Great people have great problems. Great problems make great men. Great men can be great heroes. Great heroes are the greatest problems. When you talk to people who think themselves great, they will look in their faces and tell you that they are a \"great man.\" It is easy to tell a \"great man\" that he is not a great man. It is hard to tell a \"great man\" that he is not a great man, and even harder to tell a \"\n",
            "\n",
            "Original Model example 8 -\n",
            "[{'generated_text': '“Great heroes need great sorrows and burdens, or half their greatness goes unnoticed. It is all part of the fairy tale.”\\n\\n-Anne Frank\\n\\nIt is said, “The world has enough problems as it is without worrying about the problems of your child.”\\n\\nIt is said, “You don’t have the time to worry about the problems of your child.”\\n\\nIt is true. But not entirely. And in my lifetime, I can think of many problems I have faced. But I have not worried about my child.\\n\\nFor some reason, I am much more anxious and worried about my child than about myself'}]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finetuned Model example 9 -\n",
            "“I have faith that God will show you the answer. But you have to understand that sometimes it takes a while to be able to recognize what God wants you to do. That's how it often is. God's voice is usually nothing more than a whisper, and you have to listen very carefully to hear it. But other times, in those rarest of moments, the answer is obvious and rings as loud as a church bell.”\n",
            "\n",
            "― Stephen King, The Shining\n",
            "\n",
            "\"If you don't feel it, don't do it. Don't do anything that makes you feel like you didn't enjoy it. Happiness is not just the absence of sadness. Some people will tell you that if you don't feel it, it isn't a real feeling. Well, they could have done with a little of that to themselves. I like to think of it as the feeling you get when you think of\n",
            "\n",
            "Original Model example 9 -\n",
            "[{'generated_text': \"“I have faith that God will show you the answer. But you have to understand that sometimes it takes a while to be able to recognize what God wants you to do. That's how it often is. God's voice is usually nothing more than a whisper, and you have to listen very carefully to hear it. But other times, in those rarest of moments, the answer is obvious and rings as loud as a church bell.”\"}]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    text = f\"{val_data['quote'][i]}\"\n",
        "    device = \"cuda:0\"\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "        outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "    print(f\"Finetuned Model example {i} -\")\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "    print()\n",
        "    output = pipe_og(text, max_new_tokens=100)\n",
        "    print(f\"Original Model example {i} -\")\n",
        "    print(output)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8a96e9",
      "metadata": {
        "id": "6d8a96e9",
        "outputId": "d8a4abd6-19c0-4ff8-a579-ac2040caa88d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aung San Suu Kyi\n",
            "Neil Gaiman\n",
            "J.R.R. Tolkien,\n",
            "Alexandre Dumas,\n",
            "Rainbow Rowell,\n",
            "Virginia Woolf\n",
            "Eleanor Roosevelt\n",
            "Groucho Marx\n",
            "Peter S. Beagle,\n",
            "Nicholas Sparks,\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(f\"{val_data['author'][i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6125f54f",
      "metadata": {
        "id": "6125f54f"
      },
      "source": [
        "#### Human Evaluation Rating:\n",
        "\n",
        "From 0-5 (worst to best)\n",
        "\n",
        "Example 0:\n",
        "- Finetuned Model = 4 (Gives more quotes, which is what it was trained for... even though less accurate but still is performing the task.)\n",
        "- Original Model = 3 (Explains the quote)\n",
        "    \n",
        "Example 1:\n",
        "- Finetuned Model = 1 (Repeating the quote)\n",
        "- Original Model = 3 (Couldn't guess author)\n",
        "\n",
        "Example 2:\n",
        "- Finetuned Model = 1\n",
        "- Original Model = 1 (both models are equally bad)\n",
        "\n",
        "Example 3:\n",
        "- Finetuned Model = 4 (makes similar quotes ahead)\n",
        "- Original Model = 2 (very random generation)\n",
        "\n",
        "Example 4:\n",
        "- Finetuned Model = 2 (out of context, but the language is like quotes)\n",
        "- Original Model = 0 (very out of context)\n",
        "\n",
        "Example 5:\n",
        "- Finetuned Model = 4 (generates a new quotes like sentences)\n",
        "- Original Model = 2 (out of context)\n",
        "\n",
        "Example 6:\n",
        "- Finetuned Model = 4 (generates related to context)\n",
        "- Original Model = 3 (good enough but a little off context)\n",
        "\n",
        "Example 7:\n",
        "- Finetuned Model = 5 (generates good quotes)\n",
        "- Original Model = 2 (drifts off to Eintein)\n",
        "\n",
        "Example 8:\n",
        "- Finetuned Model = 4 (some pretty deep stuff...)\n",
        "- Original Model = 1 (off context)\n",
        "\n",
        "Example 9:\n",
        "- Finetuned Model = 3\n",
        "- Original Model = 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1092426",
      "metadata": {
        "id": "e1092426"
      },
      "source": [
        "Hence we can see that the model generates more quote like text after finetuning it on a quotes dataset.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}